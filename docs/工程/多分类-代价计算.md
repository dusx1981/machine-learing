你提供的公式是机器学习中用于分类任务的 **交叉熵损失函数**，它是评估模型预测概率分布与真实分布之间差异的重要工具。下面我们分别解释这两个公式的符号含义、数学原理，并通过具体例子说明。

---

### 一、二分类交叉熵公式

$$
E = -[y \cdot \log(p) + (1 - y) \cdot \log(1 - p)]
$$

#### **符号含义：**
- $ y $：**真实标签**，取值为 0 或 1（例如：1 表示“是猫”，0 表示“不是猫”）。
- $ p $：**模型预测为正类（$ y=1 $）的概率**，取值范围在 0 到 1 之间。
- $ \log $：通常指自然对数（以 $ e $ 为底），有时也用 $ \log_2 $。
- $ E $：单个样本的交叉熵误差（损失值）。

#### **举例说明：**

假设我们要判断一张图片是否为“猫”：
- 真实标签 $ y = 1 $（是猫）
- 模型预测为猫的概率 $ p = 0.9 $

则：
$$
E = -[1 \cdot \log(0.9) + (1 - 1) \cdot \log(1 - 0.9)] = -\log(0.9) \approx 0.105
$$

如果模型预测不准，比如 $ p = 0.1 $：
$$
E = -\log(0.1) \approx 2.302
$$

**结论**：预测越准（$ p $ 越接近 $ y $），损失越小；预测越差，损失越大。

---

### 二、多分类交叉熵公式

$$
E = -\sum_{i=1}^{n} y_i \cdot \log(p_i)
$$

#### **符号含义：**
- $ n $：类别总数。
- $ y_i $：**真实标签的 one-hot 编码**，如果样本属于第 $ i $ 类，则 $ y_i = 1 $，否则为 0。
- $ p_i $：**模型预测样本属于第 $ i $ 类的概率**。
- $ E $：单个样本的交叉熵误差。

#### **举例说明：**

假设有 3 个类别：猫、狗、鸟，真实标签为“狗”（第 2 类）：
- 真实分布：$ y = [0, 1, 0] $
- 模型预测分布：$ p = [0.1, 0.7, 0.2] $

则：
$$
E = -[0 \cdot \log(0.1) + 1 \cdot \log(0.7) + 0 \cdot \log(0.2)] = -\log(0.7) \approx 0.357
$$

如果模型预测为 $ p = [0.1, 0.1, 0.8] $（预测错误）：
$$
E = -\log(0.1) \approx 2.302
$$

**结论**：模型越有信心且预测正确，损失越小；预测错误时损失急剧增大。

---

### 三、数学原理

#### 1. **信息论背景**
- **熵（Entropy）**：衡量一个随机事件的不确定性。
- **交叉熵（Cross Entropy）**：衡量两个概率分布之间的差异。
- **KL 散度（相对熵）**：交叉熵 = 熵 + KL 散度。在分类任务中，真实分布是固定的，最小化交叉熵等价于最小化 KL 散度，即让预测分布逼近真实分布。

#### 2. **与最大似然估计的关系**
- 交叉熵损失等价于**负对数似然（Negative Log-Likelihood, NLL）**。
- 最大化似然函数 ⇔ 最小化交叉熵。

#### 3. **为什么用对数？**
- 对数函数将概率映射到对数空间，使得：
  - 预测正确时（$ p \to 1 $），损失趋近于 0。
  - 预测错误时（$ p \to 0 $），损失趋近于无穷大，对错误预测施加强烈惩罚。
- 对数求导方便，便于梯度下降优化。

---

### 四、实际应用场景

- **二分类**：常用于输出层使用 Sigmoid 激活的神经网络（如垃圾邮件分类）。
- **多分类**：常用于输出层使用 Softmax 激活的神经网络（如手写数字识别、图像分类）。
- **训练目标**：通过反向传播和优化算法（如 SGD、Adam）最小化所有训练样本的平均交叉熵损失。

---

如果你需要，我可以进一步用代码展示如何实现交叉熵损失的计算，或者解释如何在 TensorFlow/PyTorch 中使用它。