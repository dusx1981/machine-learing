好的，我将通过详细的数学原理和具体例子来说明极大似然函数与代价函数之间的关系。

## 1. 基本概念关系

### 极大似然函数 vs 代价函数
- **极大似然函数**：衡量参数解释观测数据的"合理性"
- **代价函数**：衡量模型预测与真实值之间的"误差"

它们之间的关系可以表示为：
$$
\text{代价函数} = -\text{对数似然函数} + \text{常数}
$$

## 2. 数学原理推导

### 2.1 一般关系
对于独立同分布数据，似然函数：
$$
L(\theta) = \prod_{i=1}^n f(x_i \mid \theta)
$$

对数似然函数：
$$
\ell(\theta) = \sum_{i=1}^n \log f(x_i \mid \theta)
$$

代价函数通常定义为：
$$
J(\theta) = -\frac{1}{n} \ell(\theta) = -\frac{1}{n} \sum_{i=1}^n \log f(x_i \mid \theta)
$$

**核心关系**：最大化似然函数 ⇔ 最小化负对数似然函数（代价函数）

## 3. 示例1：线性回归与正态分布误差

### 3.1 问题设定
假设线性模型：$ y_i = \theta^T x_i + \varepsilon_i $，其中 $ \varepsilon_i \sim N(0, \sigma^2) $

### 3.2 似然函数推导
给定 $ x_i $，$ y_i $ 的条件分布：
$$
y_i \mid x_i \sim N(\theta^T x_i, \sigma^2)
$$

概率密度函数：
$$
f(y_i \mid x_i, \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \theta^T x_i)^2}{2\sigma^2}\right)
$$

似然函数：
$$
L(\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \theta^T x_i)^2}{2\sigma^2}\right)
$$

对数似然函数：
$$
\ell(\theta) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta^T x_i)^2
$$

### 3.3 代价函数推导
忽略常数项，负对数似然函数：
$$
-\ell(\theta) \propto \sum_{i=1}^n (y_i - \theta^T x_i)^2
$$

均方误差代价函数：
$$
J(\theta) = \frac{1}{2n} \sum_{i=1}^n (y_i - \theta^T x_i)^2
$$

**结论**：在线性回归中，最小化均方误差等价于在正态误差假设下的极大似然估计。

## 4. 示例2：逻辑回归与伯努利分布

### 4.1 问题设定
二分类问题，$ y_i \in \{0,1\} $，模型输出概率：
$$
P(y_i = 1 \mid x_i) = h_\theta(x_i) = \frac{1}{1 + e^{-\theta^T x_i}}
$$

### 4.2 似然函数推导
伯努利分布的概率质量函数：
$$
P(y_i \mid x_i) = h_\theta(x_i)^{y_i} (1 - h_\theta(x_i))^{1-y_i}
$$

似然函数：
$$
L(\theta) = \prod_{i=1}^n h_\theta(x_i)^{y_i} (1 - h_\theta(x_i))^{1-y_i}
$$

对数似然函数：
$$
\ell(\theta) = \sum_{i=1}^n \left[ y_i \log h_\theta(x_i) + (1-y_i) \log (1 - h_\theta(x_i)) \right]
$$

### 4.3 代价函数推导
负对数似然函数：
$$
J(\theta) = -\frac{1}{n} \ell(\theta) = -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log h_\theta(x_i) + (1-y_i) \log (1 - h_\theta(x_i)) \right]
$$

这就是**交叉熵损失函数**。

## 5. 具体数值例子

### 5.1 线性回归数值示例
假设有3个数据点：
- $(x_1=1, y_1=2)$，$(x_2=2, y_2=3)$，$(x_3=3, y_3=5)$
- 简单线性模型：$ y = \theta x $

**似然计算**（假设 σ=1）：
$$
L(\theta) = \prod_{i=1}^3 \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(y_i - \theta x_i)^2}{2}\right)
$$

**代价函数计算**：
$$
J(\theta) = \frac{1}{6} \left[(2-\theta)^2 + (3-2\theta)^2 + (5-3\theta)^2\right]
$$

比较不同 θ 值：

| θ | 似然 L(θ) | 代价 J(θ) | 关系 |
|---|-----------|-----------|------|
| 1.0 | 0.0010 | 2.33 | 似然小 ↔ 代价大 |
| 1.6 | 0.0032 | 0.41 | 似然大 ↔ 代价小 |
| 1.8 | 0.0021 | 0.63 | 中间值 |

### 5.2 逻辑回归数值示例
假设3个数据点：
- $(x_1=1, y_1=1)$，$(x_2=2, y_2=0)$，$(x_3=3, y_3=1)$
- 模型：$ h_\theta(x) = \frac{1}{1 + e^{-\theta x}} $

**似然计算**：
$$
L(\theta) = h_\theta(1) \times (1-h_\theta(2)) \times h_\theta(3)
$$

**代价计算**：
$$
J(\theta) = -\frac{1}{3} \left[\log h_\theta(1) + \log(1-h_\theta(2)) + \log h_\theta(3)\right]
$$

比较不同 θ 值：

| θ | h_θ(1) | h_θ(2) | h_θ(3) | 似然 L(θ) | 代价 J(θ) |
|---|--------|--------|--------|-----------|-----------|
| 0.5 | 0.62 | 0.73 | 0.82 | 0.62×0.27×0.82=0.137 | 1.28 |
| 1.0 | 0.73 | 0.88 | 0.95 | 0.73×0.12×0.95=0.083 | 1.78 |
| -0.5 | 0.38 | 0.27 | 0.18 | 0.38×0.73×0.18=0.050 | 2.12 |

## 6. 数学原理深度分析

### 6.1 信息论视角
从信息论角度看，负对数似然等于：
- 数据分布与模型分布之间的交叉熵
- 加上数据分布的熵（常数）

### 6.2 概率建模视角
代价函数的选择实际上隐含了对误差分布的假设：

| 代价函数 | 隐含的误差分布 | 应用场景 |
|----------|----------------|----------|
| 均方误差 | 正态分布 | 回归问题 |
| 交叉熵 | 伯努利/分类分布 | 分类问题 |
| 绝对误差 | 拉普拉斯分布 | 鲁棒回归 |

### 6.3 优化等价性
**定理**：对于独立同分布数据，在正则条件下：
$$
\arg\max_\theta L(\theta) = \arg\min_\theta J(\theta)
$$
其中 $ J(\theta) = -\frac{1}{n} \log L(\theta) $

## 7. 总结

极大似然函数与代价函数之间的关系可以总结为：

1. **对偶关系**：最大化似然 ⇔ 最小化负对数似然（代价函数）

2. **概率解释**：代价函数的选择对应于对数据生成过程的概率假设

3. **统一框架**：许多常见的代价函数都可以从极大似然估计推导出来

4. **实用意义**：这种关系为机器学习中的损失函数选择提供了理论依据

这种深刻的数学联系解释了为什么某些代价函数在实践中效果很好——它们对应于数据生成过程中合理的概率假设。