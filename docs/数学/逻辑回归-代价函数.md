# 逻辑回归代价函数合并表达式的数学原理详解

## 一、从分段函数到合并表达式的推导

### 1. 原始的分段代价函数

逻辑回归的代价函数最初定义为两个不同的函数：

- 当 $ y = 1 $ 时：$ \text{Cost}(h_\theta(x), 1) = -\log(h_\theta(x)) $
- 当 $ y = 0 $ 时：$ \text{Cost}(h_\theta(x), 0) = -\log(1 - h_\theta(x)) $

这两个函数可以分别绘制为：

```
y=1时的代价曲线：
   ^ Cost
   |\
   | \   (预测值h接近1时，代价接近0)
   |  \
   |   \
   |    \
   +-----+--> h
   0     1

y=0时的代价曲线：
   ^ Cost
   |    /
   |   /
   |  /   (预测值h接近0时，代价接近0)
   | /
   |/
   +-----+--> h
   0     1
```

### 2. 合并表达式的数学推导

**目标**：将上述两个分段函数合并为一个统一的表达式。

**方法1：使用伯努利分布的概率公式**

逻辑回归假设输出 $ y $ 服从伯努利分布，其概率质量函数为：
$$
P(y|p) = p^y (1-p)^{1-y}
$$
其中 $ p = h_\theta(x) $。

取对数似然：
$$
\log P(y|p) = y \log p + (1-y) \log(1-p)
$$

我们希望最大化对数似然，等价于最小化负对数似然：
$$
\text{Cost} = -[y \log p + (1-y) \log(1-p)]
$$

**验证**：
- 当 $ y = 1 $ 时：$ \text{Cost} = -[1 \cdot \log p + 0 \cdot \log(1-p)] = -\log p $
- 当 $ y = 0 $ 时：$ \text{Cost} = -[0 \cdot \log p + 1 \cdot \log(1-p)] = -\log(1-p) $

**方法2：使用指示函数和线性组合**

观察两个分段函数，我们可以将其视为一个线性组合：
$$
\text{Cost}(h, y) = y \cdot [-\log(h)] + (1-y) \cdot [-\log(1-h)]
$$
因为：
- 当 $ y = 1 $ 时，$ 1-y = 0 $，只剩下第一项
- 当 $ y = 0 $ 时，$ y = 0 $，只剩下第二项

## 二、合并表达式的数学性质

### 1. 连续性

合并表达式在 $ h $ 的定义域 $ (0,1) $ 内是连续的，且在 $ h = 0 $ 和 $ h = 1 $ 处趋于无穷大（但在定义域内，因为 $ h \in (0,1) $）。

### 2. 可微性

合并表达式对 $ h $ 是可微的：
$$
\frac{\partial \text{Cost}}{\partial h} = -\frac{y}{h} + \frac{1-y}{1-h}
$$

### 3. 凸性

对于固定的 $ y $，代价函数是关于 $ h $ 的凸函数：
$$
\frac{\partial^2 \text{Cost}}{\partial h^2} = \frac{y}{h^2} + \frac{1-y}{(1-h)^2} \geq 0
$$

## 三、实例演算

### 例1：垃圾邮件分类（二分类问题）

假设我们有两个样本：

**样本A**：$ y = 1 $（垃圾邮件），预测概率 $ h = 0.8 $
- 分段计算：$ \text{Cost} = -\log(0.8) \approx 0.2231 $
- 合并计算：$ \text{Cost} = -[1 \cdot \log(0.8) + 0 \cdot \log(0.2)] = -\log(0.8) \approx 0.2231 $

**样本B**：$ y = 0 $（正常邮件），预测概率 $ h = 0.3 $
- 分段计算：$ \text{Cost} = -\log(0.7) \approx 0.3567 $
- 合并计算：$ \text{Cost} = -[0 \cdot \log(0.3) + 1 \cdot \log(0.7)] = -\log(0.7) \approx 0.3567 $

**批量计算**：
$$
\text{Total Cost} = 0.2231 + 0.3567 = 0.5798
$$
使用合并表达式：
$$
\text{Total Cost} = -[\log(0.8) + \log(0.7)] \approx 0.5798
$$

### 例2：极端情况

**样本C**：$ y = 1 $，预测概率 $ h = 0.01 $（严重低估）
- 代价：$ -\log(0.01) = 4.6052 $

**样本D**：$ y = 0 $，预测概率 $ h = 0.99 $（严重高估）
- 代价：$ -\log(0.01) = 4.6052 $

两种情况代价都很大，符合直觉。

## 四、合并表达式的向量化实现

### 1. 矩阵表示

设有 $ m $ 个样本，特征矩阵 $ X \in \mathbb{R}^{m \times (n+1)} $，标签向量 $ y \in \mathbb{R}^m $，参数向量 $ \theta \in \mathbb{R}^{n+1} $。

预测向量：
$$
h = \sigma(X\theta) = \frac{1}{1 + e^{-X\theta}}
$$

代价函数：
$$
J(\theta) = -\frac{1}{m} \left[ y^T \log(h) + (1 - y)^T \log(1 - h) \right]
$$

### 2. Python实现示例

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, theta):
    m = len(y)
    h = sigmoid(X @ theta)  # 预测概率
    
    # 计算代价
    cost = -(1/m) * (y @ np.log(h) + (1-y) @ np.log(1-h))
    return cost

# 示例数据
X = np.array([[1, 1, 0],  # 样本1特征
              [1, 0, 1],  # 样本2特征
              [1, 0, 0]]) # 样本3特征
y = np.array([1, 0, 1])   # 标签
theta = np.array([0, 0, 0])  # 初始参数

cost = compute_cost(X, y, theta)
print(f"初始代价: {cost}")
```

## 五、合并表达式的梯度推导

### 1. 单个样本的梯度

对于单个样本 $ (x, y) $：
$$
\text{Cost} = -[y \log(h) + (1-y) \log(1-h)]
$$

其中 $ h = \sigma(z) = \sigma(\theta^T x) $。

**链式法则**：
$$
\frac{\partial \text{Cost}}{\partial \theta_j} = \frac{\partial \text{Cost}}{\partial h} \cdot \frac{\partial h}{\partial z} \cdot \frac{\partial z}{\partial \theta_j}
$$

分别计算：
1. $ \frac{\partial \text{Cost}}{\partial h} = -\frac{y}{h} + \frac{1-y}{1-h} = \frac{h-y}{h(1-h)} $
2. $ \frac{\partial h}{\partial z} = h(1-h) $
3. $ \frac{\partial z}{\partial \theta_j} = x_j $

相乘得：
$$
\frac{\partial \text{Cost}}{\partial \theta_j} = (h-y)x_j
$$

### 2. 批量梯度

对于 $ m $ 个样本：
$$
\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (h^{(i)} - y^{(i)}) x_j^{(i)}
$$

向量形式：
$$
\nabla_\theta J = \frac{1}{m} X^T (h - y)
$$

## 六、合并表达式的概率解释

### 1. 交叉熵解释

合并表达式实际上是**交叉熵损失**（Cross-Entropy Loss）：
$$
\text{Cost} = H(p, q) = -\sum_i p_i \log q_i
$$
其中 $ p $ 是真实分布，$ q $ 是预测分布。

在二分类中：
- 真实分布：$ [y, 1-y] $（one-hot编码）
- 预测分布：$ [h, 1-h] $

因此：
$$
\text{Cost} = -[y \log h + (1-y) \log(1-h)]
$$

### 2. KL散度关系

交叉熵可以分解为：
$$
H(p, q) = H(p) + D_{KL}(p \| q)
$$
其中 $ H(p) $ 是真实分布的熵，$ D_{KL} $ 是KL散度。

对于二分类，真实分布 $ p $ 的熵为0（因为 $ p $ 是确定性的），所以：
$$
\text{Cost} = D_{KL}(p \| q)
$$

## 七、合并表达式的优势

### 1. 数学简洁性

- 一个公式代替两个分段函数
- 便于求导和优化
- 易于向量化实现

### 2. 数值稳定性

在实际计算中，我们使用数值稳定版本：
```python
def stable_log_sigmoid(z):
    # 避免数值溢出
    return np.where(z >= 0,
                    -np.log(1 + np.exp(-z)),
                    z - np.log(1 + np.exp(z)))

def compute_cost_stable(X, y, theta):
    m = len(y)
    z = X @ theta
    # 分别计算正负样本的代价
    cost = -(1/m) * (y @ stable_log_sigmoid(z) + 
                     (1-y) @ stable_log_sigmoid(-z))
    return cost
```

### 3. 扩展性

合并表达式易于扩展到多分类问题（softmax回归）：
$$
\text{Cost} = -\sum_{k=1}^K y_k \log(h_k)
$$
其中 $ K $ 是类别数，$ y_k $ 是one-hot编码的真实标签，$ h_k $ 是预测概率。

## 八、代价函数的可视化

### 1. 3D可视化

考虑 $ y $ 和 $ h $ 两个变量，代价函数为：
$$
J(y, h) = -[y \log h + (1-y) \log(1-h)]
$$

这是一个在 $ (y, h) $ 平面上的曲面，其中：
- $ y \in \{0, 1\} $（离散）
- $ h \in (0, 1) $（连续）

```
代价曲面：
         Cost
          ^
          |      /  y=1
          |     /
          |    /
          |   /
          |  / 
          | /   y=0
          |/
          +---------> h
          0          1
```

### 2. 等高线图

固定 $ y $，绘制代价随 $ h $ 变化的曲线：
- 当 $ y = 1 $：代价随 $ h $ 减小而增大
- 当 $ y = 0 $：代价随 $ h $ 增大而增大

## 九、实际应用中的注意事项

### 1. 处理 $ h = 0 $ 或 $ h = 1 $ 的情况

理论上 $ h \in (0, 1) $，但数值计算中可能接近边界。常用的技巧是添加一个小的epsilon：
```python
epsilon = 1e-15
h = np.clip(h, epsilon, 1-epsilon)
cost = -[y * np.log(h) + (1-y) * np.log(1-h)]
```

### 2. 类别不平衡问题

当正负样本比例失衡时，可以给不同类别加权：
$$
\text{Cost} = -[w_1 \cdot y \log h + w_0 \cdot (1-y) \log(1-h)]
$$
其中 $ w_1, w_0 $ 是类别权重。

### 3. 正则化

加入L2正则化：
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log h^{(i)} + (1-y^{(i)}) \log(1-h^{(i)})] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2
$$

## 十、总结

逻辑回归代价函数的合并表达式：
$$
\text{Cost}(h, y) = -[y \log h + (1-y) \log(1-h)]
$$

**数学原理总结**：
1. **推导基础**：基于伯努利分布的最大似然估计
2. **等价性**：与分段函数完全等价
3. **数学性质**：连续、可微、凸性
4. **概率解释**：交叉熵损失，衡量预测分布与真实分布的差异
5. **优化优势**：梯度形式简单，易于优化
6. **扩展性**：可扩展到多分类和加权情况

合并表达式不仅简化了数学表达和计算实现，而且为理解逻辑回归的概率本质提供了清晰的框架。它连接了概率论、信息论和优化理论，是逻辑回归模型的核心数学基础之一。