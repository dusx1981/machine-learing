### 最小二乘法推导过程（基于矩阵微分）

#### 1. 问题表述
设有线性模型：
$$
\mathbf{y} = \mathbf{X} \boldsymbol{\theta} + \boldsymbol{\varepsilon}
$$
其中：
- $\mathbf{y} \in \mathbb{R}^{n \times 1}$：观测向量；
- $\mathbf{X} \in \mathbb{R}^{n \times p}$：设计矩阵（已知）；
- $\boldsymbol{\theta} \in \mathbb{R}^{p \times 1}$：待估参数向量；
- $\boldsymbol{\varepsilon} \in \mathbb{R}^{n \times 1}$：误差向量。

目标：最小化残差平方和损失函数：
$$
J(\boldsymbol{\theta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\theta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\theta}).
$$

#### 2. 展开损失函数
利用转置运算性质：
- $(\mathbf{A} + \mathbf{B})^\top = \mathbf{A}^\top + \mathbf{B}^\top$；
- $(\mathbf{A}\mathbf{B})^\top = \mathbf{B}^\top \mathbf{A}^\top$。

令 $\mathbf{e} = \mathbf{y} - \mathbf{X}\boldsymbol{\theta}$，则：
$$
\begin{aligned}
J(\boldsymbol{\theta}) &= \mathbf{e}^\top \mathbf{e} \\
&= (\mathbf{y} - \mathbf{X}\boldsymbol{\theta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\theta}) \\
&= \left( \mathbf{y}^\top - (\mathbf{X}\boldsymbol{\theta})^\top \right) (\mathbf{y} - \mathbf{X}\boldsymbol{\theta}) \quad \text{(利用加法转置)} \\
&= \left( \mathbf{y}^\top - \boldsymbol{\theta}^\top \mathbf{X}^\top \right) (\mathbf{y} - \mathbf{X}\boldsymbol{\theta}) \quad \text{(利用乘法转置)}.
\end{aligned}
$$

展开乘积：
$$
\begin{aligned}
J(\boldsymbol{\theta}) &= \mathbf{y}^\top \mathbf{y} - \mathbf{y}^\top \mathbf{X}\boldsymbol{\theta} - \boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{y} + \boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{X}\boldsymbol{\theta}.
\end{aligned}
$$

注意 $\mathbf{y}^\top \mathbf{X}\boldsymbol{\theta}$ 是一个标量，因此其转置等于自身：
$$
\mathbf{y}^\top \mathbf{X}\boldsymbol{\theta} = (\mathbf{y}^\top \mathbf{X}\boldsymbol{\theta})^\top = \boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{y}.
$$
所以中间两项相等，合并为：
$$
J(\boldsymbol{\theta}) = \mathbf{y}^\top \mathbf{y} - 2\boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{y} + \boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{X}\boldsymbol{\theta}.
$$

#### 3. 矩阵微分求梯度
对 $J(\boldsymbol{\theta})$ 关于 $\boldsymbol{\theta}$ 求导，使用以下矩阵导数公式（标量对向量求导）：
- $\dfrac{\partial (\boldsymbol{\theta}^\top \mathbf{a})}{\partial \boldsymbol{\theta}} = \mathbf{a}$，其中 $\mathbf{a}$ 为常数向量；
- $\dfrac{\partial (\boldsymbol{\theta}^\top \mathbf{A} \boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = (\mathbf{A} + \mathbf{A}^\top)\boldsymbol{\theta}$，若 $\mathbf{A}$ 对称则等于 $2\mathbf{A}\boldsymbol{\theta}$。

此处 $\mathbf{X}^\top \mathbf{X}$ 是对称矩阵，故：
$$
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{\theta}} (\boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{y}) &= \mathbf{X}^\top \mathbf{y}, \\
\frac{\partial}{\partial \boldsymbol{\theta}} (\boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{X}\boldsymbol{\theta}) &= 2\mathbf{X}^\top \mathbf{X}\boldsymbol{\theta}.
\end{aligned}
$$
常数项 $\mathbf{y}^\top \mathbf{y}$ 导数为零。因此：
$$
\frac{\partial J}{\partial \boldsymbol{\theta}} = 0 - 2\mathbf{X}^\top \mathbf{y} + 2\mathbf{X}^\top \mathbf{X}\boldsymbol{\theta} = -2\mathbf{X}^\top \mathbf{y} + 2\mathbf{X}^\top \mathbf{X}\boldsymbol{\theta}.
$$

#### 4. 求解最优参数
为最小化 $J(\boldsymbol{\theta})$，令梯度为零：
$$
\frac{\partial J}{\partial \boldsymbol{\theta}} = \mathbf{0} \quad \Rightarrow \quad -2\mathbf{X}^\top \mathbf{y} + 2\mathbf{X}^\top \mathbf{X}\boldsymbol{\theta} = \mathbf{0}.
$$
两边除以 2 并移项得正规方程：
$$
\mathbf{X}^\top \mathbf{X} \boldsymbol{\theta} = \mathbf{X}^\top \mathbf{y}.
$$
假设 $\mathbf{X}^\top \mathbf{X}$ 可逆（即 $\mathbf{X}$ 列满秩），解得：
$$
\boldsymbol{\theta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}.
$$

#### 5. 数学原理与解释
- **几何视角**：最小二乘估计将观测向量 $\mathbf{y}$ 投影到设计矩阵 $\mathbf{X}$ 的列空间上，投影向量 $\mathbf{X}\hat{\boldsymbol{\theta}}$ 是最佳逼近，残差向量 $\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\theta}}$ 与 $\mathbf{X}$ 的列空间正交，即 $\mathbf{X}^\top (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\theta}}) = \mathbf{0}$，这正是正规方程。
- **统计视角**：若误差满足零均值、同方差、无自相关且与解释变量不相关，则最小二乘估计是 $\boldsymbol{\theta}$ 的最佳线性无偏估计（BLUE）。
- **计算优势**：直接通过矩阵运算得到解析解，无需迭代优化，效率高且稳定（当 $\mathbf{X}^\top \mathbf{X}$ 条件数较小时）。

此推导清晰展示了从损失函数定义到参数估计的完整过程，涵盖了矩阵转置性质、微分法则和最优化条件，是线性回归理论的核心基础。