# 从几率比(Odds)到逻辑回归的详细推导

## 一、基本定义与概念

### 1. 几率和对数几率

对于二分类问题（类别1和0），给定特征x时，事件发生的**几率(Odds)**定义为：

$$
\text{Odds} = \frac{P}{1-P}
$$

其中：
- $ P = P(y=1|x) $ 是事件发生的概率
- $ 1-P = P(y=0|x) $ 是事件不发生的概率

**例子：** 如果$P=0.8$，则$\text{Odds} = 0.8/0.2 = 4$，表示"发生"的可能性是"不发生"的4倍。

### 2. 对数几率(Logit函数)

对几率取自然对数，得到**对数几率(Logit)**：

$$
\text{logit}(P) = \ln\left[\frac{P}{1-P}\right]
$$

## 二、关键假设与推导

### 步骤1：建立线性关系假设

逻辑回归的核心假设是：**对数几率是输入特征的线性组合**。

设特征向量为$x$，权重向量为$w$，偏置为$b$，则有：

$$
\ln\left[\frac{P}{1-P}\right] = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b
$$

简写为：

$$
\ln\left[\frac{P}{1-P}\right] = w^\top x + b
$$

**为什么这个假设合理？**
1. 线性模型简单且可解释
2. 对数几率值域为$(-\infty, +\infty)$，与线性输出值域匹配
3. 保持了事件发生概率的单调性

### 步骤2：从对数几率反推概率

令：

$$
z = w^\top x + b
$$

则有：

$$
\ln\left[\frac{P}{1-P}\right] = z
$$

两边取指数：

$$
\frac{P}{1-P} = e^z
$$

### 步骤3：求解概率P

由上式：

$$
P = e^z(1-P)
$$
$$
P = e^z - e^zP
$$
$$
P + e^zP = e^z
$$
$$
P(1 + e^z) = e^z
$$

解得：

$$
P = \frac{e^z}{1 + e^z}
$$

### 步骤4：引入Sigmoid函数

将分子分母同时除以$e^z$：

$$
P = \frac{1}{1 + e^{-z}}
$$

代入$z = w^\top x + b$：

$$
P(y=1|x) = \frac{1}{1 + e^{-(w^\top x + b)}}
$$

这就是**Sigmoid函数**：

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

## 三、Sigmoid函数的性质

### 1. 函数图像

$$
\begin{aligned}
& z \to -\infty: \sigma(z) \to 0 \\
& z = 0: \sigma(z) = 0.5 \\
& z \to +\infty: \sigma(z) \to 1
\end{aligned}
$$

### 2. 导数特性

Sigmoid函数的导数为：

$$
\frac{d\sigma(z)}{dz} = \sigma(z)[1 - \sigma(z)]
$$

这个性质在梯度计算中非常重要。

## 四、最大似然估计推导

### 步骤1：建立概率模型

对于第$i$个样本，观测到标签$y_i$的概率为：
- 如果$y_i = 1$：$P(y_i=1|x_i) = \sigma(w^\top x_i + b)$
- 如果$y_i = 0$：$P(y_i=0|x_i) = 1 - \sigma(w^\top x_i + b)$

合并为一个表达式：

$$
P(y_i|x_i; w, b) = [\sigma(z_i)]^{y_i} \times [1 - \sigma(z_i)]^{1-y_i}
$$

其中 $z_i = w^\top x_i + b$

### 步骤2：构建似然函数

对于$N$个独立同分布的样本，联合概率（似然函数）为：

$$
L(w, b) = \prod_{i=1}^N [\sigma(z_i)]^{y_i} [1 - \sigma(z_i)]^{1-y_i}
$$

### 步骤3：取对数似然

为方便优化，取自然对数：

$$
\ell(w, b) = \ln L(w, b) = \sum_{i=1}^N \left[ y_i \ln \sigma(z_i) + (1-y_i) \ln(1 - \sigma(z_i)) \right]
$$

### 步骤4：推导损失函数（负对数似然）

通常我们最小化负对数似然（损失函数）：

$$
J(w, b) = -\ell(w, b) = -\sum_{i=1}^N \left[ y_i \ln \sigma(z_i) + (1-y_i) \ln(1 - \sigma(z_i)) \right]
$$

这就是**二元交叉熵损失函数**。

## 五、梯度计算与参数更新

### 步骤1：计算梯度

对权重$w_j$的偏导数为：

$$
\frac{\partial J}{\partial w_j} = \sum_{i=1}^N (\sigma(z_i) - y_i)x_{ij}
$$

**推导过程：**

令 $\sigma_i = \sigma(z_i)$

$$
\begin{aligned}
\frac{\partial J}{\partial w_j} &= -\sum \left[ y_i \frac{1}{\sigma_i} \frac{\partial \sigma_i}{\partial w_j} + (1-y_i) \frac{-1}{1-\sigma_i} \frac{\partial \sigma_i}{\partial w_j} \right] \\
&= -\sum \left[ \frac{y_i}{\sigma_i} - \frac{1-y_i}{1-\sigma_i} \right] \frac{\partial \sigma_i}{\partial w_j}
\end{aligned}
$$

由于 $\frac{\partial \sigma_i}{\partial w_j} = \sigma_i(1-\sigma_i)x_{ij}$

代入得：

$$
\begin{aligned}
\frac{\partial J}{\partial w_j} &= -\sum \left[ y_i(1-\sigma_i) - (1-y_i)\sigma_i \right] x_{ij} \\
&= -\sum (y_i - \sigma_i) x_{ij} \\
&= \sum (\sigma_i - y_i) x_{ij}
\end{aligned}
$$

### 步骤2：参数更新规则

使用梯度下降法：

$$
w_j \leftarrow w_j - \alpha \times \frac{\partial J}{\partial w_j} = w_j - \alpha \times \sum_{i=1}^N (\sigma(z_i) - y_i)x_{ij}
$$

其中$\alpha$是学习率。

## 六、实际例子：肿瘤分类

### 问题描述
根据肿瘤大小预测是否为恶性（1=恶性，0=良性）

### 数据

| 肿瘤大小(x) | 恶性(y) |
|------------|--------|
| 1.0        | 0      |
| 1.5        | 0      |
| 2.0        | 0      |
| 2.5        | 1      |
| 3.0        | 1      |
| 3.5        | 1      |
| 4.0        | 1      |

### 推导应用

1. **模型假设**：
   $$
   \ln\left[\frac{P}{1-P}\right] = w \cdot x + b
   $$
   其中$P = P(\text{恶性}|\text{肿瘤大小})$

2. **求解过程**：
   假设通过训练得到：$w = 1.8, b = -4.5$

3. **预测新样本**：
   对于$x = 2.2$ cm：
   $$
   z = 1.8 \times 2.2 - 4.5 = -0.54
   $$
   $$
   P = \frac{1}{1+e^{0.54}} \approx 0.37
   $$
   预测为良性（因为$P < 0.5$）

## 七、广义推导：多特征情况

对于$n$个特征的情况：

$$
\ln\left[\frac{P}{1-P}\right] = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b
$$

解出：

$$
P = \frac{1}{1 + e^{-(w_1x_1 + w_2x_2 + \cdots + w_nx_n + b)}}
$$

决策边界是超平面：

$$
w_1x_1 + w_2x_2 + \cdots + w_nx_n + b = 0
$$

## 八、为什么选择对数几率？

### 数学合理性：
1. **值域匹配**：对数几率值域为$(-\infty, +\infty)$，与线性函数值域一致
2. **单调性**：保持概率的单调关系
3. **可逆性**：可从对数几率唯一确定概率

### 统计合理性：
1. **充分统计量**：在指数族分布中，对数几率是自然参数
2. **大样本性质**：最大似然估计具有渐近正态性等优良性质

## 九、逻辑回归与广义线性模型

逻辑回归是**广义线性模型(GLM)**的特例：
1. **随机成分**：$y$服从伯努利分布
2. **系统成分**：线性预测器$\eta = w^\top x + b$
3. **连接函数**：logit函数，$g(\mu) = \ln\left[\frac{\mu}{1-\mu}\right]$

其他GLM通过不同连接函数得到：
- 正态分布+恒等连接 → 线性回归
- 泊松分布+对数连接 → 泊松回归

## 十、总结推导路径

```
事件概率P
    ↓ 定义
几率 Odds = P/(1-P)
    ↓ 取对数
对数几率 ln[P/(1-P)]
    ↓ 关键假设：线性关系
ln[P/(1-P)] = wᵀx + b
    ↓ 代数变换
P = 1/(1 + e^{-(wᵀx + b)})
    ↓ 定义Sigmoid函数
P = σ(wᵀx + b)
    ↓ 构建似然函数
L(w,b) = ∏ [σ(zᵢ)]^{yᵢ}[1-σ(zᵢ)]^{1-yᵢ}
    ↓ 最大似然估计
最大化 ℓ(w,b) = ∑[yᵢ ln σ(zᵢ) + (1-yᵢ) ln(1-σ(zᵢ))]
    ↓ 等价于
最小化 J(w,b) = -ℓ(w,b)  （交叉熵损失）
```

这个推导过程展示了逻辑回归如何自然地从几率比概念中产生，并通过统计建模方法得到参数估计。它连接了概率论、统计学和机器学习，提供了一个从线性回归扩展到分类问题的优雅框架。