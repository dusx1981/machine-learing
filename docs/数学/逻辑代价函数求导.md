# 逻辑回归代价函数梯度推导详解

## 一、基本符号与公式

### 1. 符号说明

| 符号 | 含义 | 维度 |
|------|------|------|
| $ m $ | 样本数量 | 标量 |
| $ n $ | 特征数量（不包括偏置项） | 标量 |
| $ x^{(i)} $ | 第 $ i $ 个样本的特征向量 | $ (n+1) \times 1 $ |
| $ y^{(i)} $ | 第 $ i $ 个样本的标签 | $ \{0, 1\} $ |
| $ \theta $ | 模型参数向量 | $ (n+1) \times 1 $ |
| $ \theta_j $ | 第 $ j $ 个参数 | 标量 |
| $ h_\theta(x) $ | 逻辑回归预测函数 | $ (0, 1) $ |
| $ J(\theta) $ | 代价函数 | 标量 |

### 2. 关键公式

1. **逻辑回归函数（Sigmoid）**：
   $$
   h_\theta(x) = \sigma(z) = \frac{1}{1 + e^{-z}}, \quad z = \theta^T x = \sum_{j=0}^n \theta_j x_j
   $$

2. **代价函数（交叉熵损失）**：
   $$
   J(\theta) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]
   $$

3. **Sigmoid函数的导数**：
   $$
   \frac{d\sigma(z)}{dz} = \sigma(z)(1 - \sigma(z)) = h(1-h)
   $$

## 二、梯度推导过程

### 1. 目标：求 $ \frac{\partial J(\theta)}{\partial \theta_j} $

我们需要计算代价函数对每个参数 $ \theta_j $ 的偏导数。

### 2. 单个样本的推导

先考虑单个样本 $ (x, y) $ 的损失：
$$
\text{Cost}(\theta) = -[y \log(h) + (1-y) \log(1-h)]
$$
其中 $ h = h_\theta(x) = \sigma(\theta^T x) $。

**步骤1：应用链式法则**

$$
\frac{\partial \text{Cost}}{\partial \theta_j} = \frac{\partial \text{Cost}}{\partial h} \cdot \frac{\partial h}{\partial z} \cdot \frac{\partial z}{\partial \theta_j}
$$
其中 $ z = \theta^T x $。

**步骤2：计算每个部分**

1. **第一部分：$ \frac{\partial \text{Cost}}{\partial h} $**
   $$
   \begin{aligned}
   \frac{\partial \text{Cost}}{\partial h} &= \frac{\partial}{\partial h} \left[ -y \log h - (1-y) \log(1-h) \right] \\
   &= -\frac{y}{h} + \frac{1-y}{1-h} \\
   &= \frac{-y(1-h) + (1-y)h}{h(1-h)} \\
   &= \frac{-y + yh + h - yh}{h(1-h)} \\
   &= \frac{h - y}{h(1-h)}
   \end{aligned}
   $$

2. **第二部分：$ \frac{\partial h}{\partial z} $**
   $$
   \frac{\partial h}{\partial z} = h(1-h)
   $$
   （Sigmoid函数的导数性质）

3. **第三部分：$ \frac{\partial z}{\partial \theta_j} $**
   $$
   \frac{\partial z}{\partial \theta_j} = \frac{\partial}{\partial \theta_j} (\theta^T x) = x_j
   $$

**步骤3：组合结果**

$$
\begin{aligned}
\frac{\partial \text{Cost}}{\partial \theta_j} &= \frac{h - y}{h(1-h)} \cdot h(1-h) \cdot x_j \\
&= (h - y) x_j
\end{aligned}
$$

**奇迹般的简化**：中间项 $ h(1-h) $ 正好约掉！

### 3. 扩展到所有样本

对于所有 $ m $ 个样本，代价函数是平均损失：
$$
J(\theta) = \frac{1}{m} \sum_{i=1}^m \text{Cost}^{(i)}(\theta)
$$

因此：
$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
$$

### 4. 向量形式

令：
- $ X \in \mathbb{R}^{m \times (n+1)} $：设计矩阵（每行一个样本）
- $ y \in \mathbb{R}^m $：标签向量
- $ h = \sigma(X\theta) \in \mathbb{R}^m $：预测向量

则梯度向量为：
$$
\nabla_\theta J(\theta) = \frac{1}{m} X^T (h - y)
$$

## 三、实例演算

### 场景：垃圾邮件分类（续前例）

假设我们有3个样本，2个特征（加上偏置项，共3个参数）：

#### 1. 数据准备

**特征矩阵 $ X $（已添加偏置项 $ x_0 = 1 $）**：
$$
X = \begin{bmatrix}
1 & 1 & 0 \\  # 样本1：包含"免费"
1 & 0 & 1 \\  # 样本2：包含"会议"
1 & 1 & 1 \\  # 样本3：包含"免费"和"会议"
\end{bmatrix}
$$

**标签向量 $ y $**：
$$
y = \begin{bmatrix}
1 \\  # 垃圾邮件
0 \\  # 正常邮件
1 \\  # 垃圾邮件
\end{bmatrix}
$$

#### 2. 初始化参数

$$
\theta = \begin{bmatrix}
\theta_0 \\ \theta_1 \\ \theta_2
\end{bmatrix} = \begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}
$$

#### 3. 计算预测值 $ h $

对于每个样本 $ i $：
$$
h^{(i)} = \sigma(\theta^T x^{(i)})
$$

计算：
- 样本1：$ z^{(1)} = 0\cdot1 + 0\cdot1 + 0\cdot0 = 0 $，$ h^{(1)} = \sigma(0) = 0.5 $
- 样本2：$ z^{(2)} = 0\cdot1 + 0\cdot0 + 0\cdot1 = 0 $，$ h^{(2)} = \sigma(0) = 0.5 $
- 样本3：$ z^{(3)} = 0\cdot1 + 0\cdot1 + 0\cdot1 = 0 $，$ h^{(3)} = \sigma(0) = 0.5 $

所以：
$$
h = \begin{bmatrix}
0.5 \\ 0.5 \\ 0.5
\end{bmatrix}
$$

#### 4. 计算误差向量

$$
h - y = \begin{bmatrix}
0.5 - 1 \\ 0.5 - 0 \\ 0.5 - 1
\end{bmatrix} = \begin{bmatrix}
-0.5 \\ 0.5 \\ -0.5
\end{bmatrix}
$$

#### 5. 计算梯度

**方法1：按公式计算**

对于 $ \theta_0 $：
$$
\frac{\partial J}{\partial \theta_0} = \frac{1}{3} \sum_{i=1}^3 (h^{(i)} - y^{(i)}) \cdot 1
= \frac{1}{3} (-0.5 + 0.5 - 0.5) = \frac{1}{3} (-0.5) = -\frac{1}{6}
$$

对于 $ \theta_1 $：
$$
\frac{\partial J}{\partial \theta_1} = \frac{1}{3} \sum_{i=1}^3 (h^{(i)} - y^{(i)}) x_1^{(i)}
= \frac{1}{3} [(-0.5 \cdot 1) + (0.5 \cdot 0) + (-0.5 \cdot 1)]
= \frac{1}{3} (-0.5 - 0.5) = \frac{1}{3} (-1) = -\frac{1}{3}
$$

对于 $ \theta_2 $：
$$
\frac{\partial J}{\partial \theta_2} = \frac{1}{3} \sum_{i=1}^3 (h^{(i)} - y^{(i)}) x_2^{(i)}
= \frac{1}{3} [(-0.5 \cdot 0) + (0.5 \cdot 1) + (-0.5 \cdot 1)]
= \frac{1}{3} (0.5 - 0.5) = 0
$$

**方法2：用矩阵形式验证**

$$
X^T (h - y) = \begin{bmatrix}
1 & 1 & 1 \\
1 & 0 & 1 \\
0 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
-0.5 \\ 0.5 \\ -0.5
\end{bmatrix}
= \begin{bmatrix}
-0.5 \\ -1 \\ 0
\end{bmatrix}
$$

除以 $ m = 3 $：
$$
\nabla_\theta J = \frac{1}{3} \begin{bmatrix}
-0.5 \\ -1 \\ 0
\end{bmatrix} = \begin{bmatrix}
-\frac{1}{6} \\ -\frac{1}{3} \\ 0
\end{bmatrix}
$$

两种方法结果一致！

#### 6. 参数更新（梯度下降）

假设学习率 $ \alpha = 0.1 $：

更新规则：$ \theta := \theta - \alpha \nabla_\theta J $

$$
\theta_{\text{new}} = \begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix} - 0.1 \cdot \begin{bmatrix}
-1/6 \\ -1/3 \\ 0
\end{bmatrix}
= \begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix} + \begin{bmatrix}
1/60 \\ 1/30 \\ 0
\end{bmatrix}
= \begin{bmatrix}
0.0167 \\ 0.0333 \\ 0
\end{bmatrix}
$$

## 四、梯度推导的几何解释

### 1. 梯度方向的意义

梯度 $ \nabla_\theta J $ 指向代价函数增长最快的方向。在梯度下降中，我们朝负梯度方向更新参数，以减小代价。

### 2. 误差项的直观理解

梯度公式中的 $ (h - y) $ 项是**预测误差**：
- 当 $ h > y $ 时（预测概率太高），误差为正，梯度为正，我们需要减小 $ \theta $ 来降低预测值
- 当 $ h < y $ 时（预测概率太低），误差为负，梯度为负，我们需要增加 $ \theta $ 来提高预测值

### 3. 学习过程可视化

```
参数空间中的梯度下降：
         J(θ)
          ^
          | \
          |  \
          |   \
          |    \
          |     \
          +-------> θ
          
梯度方向：最陡下降方向
更新：θ ← θ - α·∇J
```

## 五、正则化情况下的梯度

### 1. L2正则化的代价函数

$$
J_{\text{reg}}(\theta) = J(\theta) + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2
$$
（注意：通常不对 $ \theta_0 $ 正则化）

### 2. 正则化梯度

对于 $ j = 0 $：
$$
\frac{\partial J_{\text{reg}}}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^m (h^{(i)} - y^{(i)}) x_0^{(i)}
$$

对于 $ j \geq 1 $：
$$
\frac{\partial J_{\text{reg}}}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (h^{(i)} - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} \theta_j
$$

### 3. 向量形式

$$
\nabla_\theta J_{\text{reg}} = \frac{1}{m} X^T (h - y) + \frac{\lambda}{m} \theta'
$$
其中 $ \theta' $ 是 $ \theta $ 去掉 $ \theta_0 $ 后的向量（在对应位置补0）。

## 六、梯度推导中的数学技巧

### 1. 链式法则的简化

注意到：
$$
\frac{\partial \text{Cost}}{\partial \theta_j} = \frac{\partial \text{Cost}}{\partial h} \cdot \frac{\partial h}{\partial z} \cdot \frac{\partial z}{\partial \theta_j}
$$
中间项 $ \frac{\partial h}{\partial z} = h(1-h) $ 恰好被 $ \frac{\partial \text{Cost}}{\partial h} $ 中的分母消去，这是交叉熵损失与Sigmoid函数搭配的**神奇性质**。

### 2. 与其他损失函数的比较

如果用均方误差（MSE）作为损失函数：
$$
\text{Cost}_{\text{MSE}} = \frac{1}{2}(h - y)^2
$$
则梯度为：
$$
\frac{\partial \text{Cost}_{\text{MSE}}}{\partial \theta_j} = (h - y) \cdot h(1-h) \cdot x_j
$$
多了 $ h(1-h) $ 项，可能导致梯度消失问题（当 $ h $ 接近0或1时，梯度很小）。

## 七、数值稳定性考虑

### 1. 防止数值溢出

在实际计算中，直接计算 $ \sigma(z) $ 可能溢出。常用技巧：

```python
def sigmoid_stable(z):
    if z >= 0:
        return 1 / (1 + np.exp(-z))
    else:
        # 避免负数指数溢出
        exp_z = np.exp(z)
        return exp_z / (1 + exp_z)
```

### 2. 对数损失的计算

为了避免 $ \log(0) $ 问题：
```python
epsilon = 1e-15
h = np.clip(h, epsilon, 1 - epsilon)
cost = -[y * np.log(h) + (1-y) * np.log(1-h)]
```

## 八、总结：梯度推导的关键点

1. **核心公式**：
   $$
   \frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
   $$

2. **推导步骤**：
   - 应用链式法则
   - 计算三个偏导数
   - 利用Sigmoid函数的导数性质简化

3. **几何意义**：梯度指向代价函数增长最快的方向

4. **实际应用**：
   - 用于梯度下降算法更新参数
   - 向量化实现提高计算效率
   - 正则化防止过拟合

逻辑回归的梯度推导展示了数学的优美：虽然代价函数看似复杂，但其梯度却异常简洁。这种简洁性使得逻辑回归在实际应用中非常高效，也是深度学习反向传播算法的基础。