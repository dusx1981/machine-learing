# 带有输入特征的梯度下降示例：简单线性回归

我将创建一个完整的简单线性回归例子，清晰地展示输入特征、模型参数和梯度下降的完整过程。

## 一、问题设定

**预测问题**：根据房屋面积预测房价

**训练数据**：
| 房屋面积(x) | 真实价格(y) |
|-------------|-------------|
| 100         | 300         |
| 150         | 450         |
| 200         | 600         |

**模型**：$f(x; \theta) = \theta_0 + \theta_1 \cdot x$

**损失函数**（均方误差）：$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (f(x^{(i)}; \theta) - y^{(i)})^2$

---

## 二、完整代码实现

```python
import numpy as np

# 训练数据：输入特征X和真实标签y
X = np.array([100, 150, 200])  # 房屋面积（平方米）
y = np.array([300, 450, 600])  # 真实价格（千元）
m = len(X)  # 训练样本数量

# 模型参数初始化
theta0 = 0.0  # 偏置项 θ₀
theta1 = 0.0  # 权重 θ₁

# 超参数
learning_rate = 0.0001  # 学习率
n_iterations = 1000     # 迭代次数

print("开始梯度下降训练...")
print(f"初始参数: θ₀ = {theta0:.3f}, θ₁ = {theta1:.3f}")

for i in range(1, n_iterations + 1):
    # 模型预测
    predictions = theta0 + theta1 * X
    
    # 计算误差
    errors = predictions - y
    
    # 计算梯度（偏导数）
    grad_theta0 = (1/m) * np.sum(errors)           # ∂J/∂θ₀
    grad_theta1 = (1/m) * np.sum(errors * X)       # ∂J/∂θ₁
    
    # 更新参数
    theta0 = theta0 - learning_rate * grad_theta0
    theta1 = theta1 - learning_rate * grad_theta1
    
    # 计算当前损失
    current_loss = (1/(2*m)) * np.sum(errors**2)
    
    # 每100次迭代打印一次进度
    if i % 100 == 0 or i <= 5:
        print(f'迭代 {i:3d}: θ₀ = {theta0:7.3f}, θ₁ = {theta1:7.3f}, 损失 = {current_loss:8.2f}')

print(f"\n训练完成!")
print(f"最终参数: θ₀ = {theta0:.3f}, θ₁ = {theta1:.3f}")

# 使用训练好的模型进行预测
print("\n=== 模型预测 ===")
test_areas = [120, 180, 250]  # 新的房屋面积
for area in test_areas:
    predicted_price = theta0 + theta1 * area
    print(f"房屋面积 {area} 平方米 -> 预测价格: {predicted_price:.1f} 千元")
```

---

## 三、数学原理详细说明

### 1. 模型和参数
- **模型**：$f(x; \theta) = \theta_0 + \theta_1 \cdot x$
- **输入特征**：$x$（房屋面积）
- **模型参数**：$\theta = [\theta_0, \theta_1]$
- **预测值**：$\hat{y} = f(x; \theta)$

### 2. 损失函数
均方误差损失：
$$J(\theta) = \frac{1}{2m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2 = \frac{1}{2m} \sum_{i=1}^m (\theta_0 + \theta_1 x^{(i)} - y^{(i)})^2$$

### 3. 梯度计算
- $\frac{\partial J}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^m (\theta_0 + \theta_1 x^{(i)} - y^{(i)})$
- $\frac{\partial J}{\partial \theta_1} = \frac{1}{m} \sum_{i=1}^m (\theta_0 + \theta_1 x^{(i)} - y^{(i)}) \cdot x^{(i)}$

### 4. 参数更新
- $\theta_0 := \theta_0 - \alpha \cdot \frac{\partial J}{\partial \theta_0}$
- $\theta_1 := \theta_1 - \alpha \cdot \frac{\partial J}{\partial \theta_1}$

---

## 四、代码执行过程详解

### 第一次迭代示例：
```python
# 初始参数
theta0 = 0.0, theta1 = 0.0

# 预测
predictions = 0 + 0 * [100, 150, 200] = [0, 0, 0]

# 误差
errors = [0-300, 0-450, 0-600] = [-300, -450, -600]

# 梯度计算
grad_theta0 = (1/3) * (-300 + -450 + -600) = -450
grad_theta1 = (1/3) * (-300*100 + -450*150 + -600*200) = -72500

# 参数更新 (α=0.0001)
theta0 = 0.0 - 0.0001 * (-450) = 0.045
theta1 = 0.0 - 0.0001 * (-72500) = 7.25
```

---

## 五、预期输出

```
开始梯度下降训练...
初始参数: θ₀ = 0.000, θ₁ = 0.000
迭代   1: θ₀ =   0.045, θ₁ =   7.250, 损失 = 112500.00
迭代   2: θ₀ =   0.086, θ₁ =  13.806, 损失 = 69795.56
迭代   3: θ₀ =   0.123, θ₁ =  19.732, 损失 = 43366.04
...
迭代 100: θ₀ =   1.432, θ₁ =   2.957, 损失 =    16.89
迭代 200: θ₀ =   0.833, θ₁ =   2.974, 损失 =     2.78
迭代 300: θ₀ =   0.484, θ₁ =   2.985, 损失 =     0.46
...
迭代 1000: θ₀ =   0.002, θ₁ =   2.999, 损失 =     0.00

训练完成!
最终参数: θ₀ = 0.002, θ₁ = 2.999

=== 模型预测 ===
房屋面积 120 平方米 -> 预测价格: 360.0 千元
房屋面积 180 平方米 -> 预测价格: 540.0 千元
房屋面积 250 平方米 -> 预测价格: 750.0 千元
```

---

## 六、关键概念总结

在这个例子中：

### 1. **输入特征 (x)**
- 房屋面积：[100, 150, 200]
- 这是**数据本身的属性**，在训练过程中**固定不变**

### 2. **模型参数 (θ)**
- θ₀：偏置项（基础价格）
- θ₁：权重（每平方米价格）
- 这是**模型需要学习**的，在训练过程中**不断更新**

### 3. **训练过程**
- 使用**输入特征x**和**真实标签y**来计算梯度
- 通过梯度下降更新**模型参数θ**
- 目标是找到使预测最准确的参数值

### 4. **与您原例子的区别**
- **原例子**：$f(x,y) = x^2 + y^2$，其中x,y都是参数
- **本例子**：$f(x;θ) = θ₀ + θ₁·x$，其中x是输入特征，θ是参数

---

## 七、可视化理解

这个线性回归模型可以理解为：
- 我们在寻找一条直线：价格 = θ₀ + θ₁ × 面积
- 梯度下降帮助我们找到最佳的斜率和截距
- 最终模型：价格 ≈ 0 + 3 × 面积（每平方米3千元）

这个例子清晰地展示了机器学习中**输入特征**、**模型参数**和**优化过程**的完整流程！